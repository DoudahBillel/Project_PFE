{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Brain Atlas through Deep Learning \n",
    "## A. Iqbal, R. Khan, T. Karayannis\n",
    "### Source : https://github.com/itsasimiqbal/SeBRe\n",
    "\n",
    "## Modified by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "os.chdir('/home/hamza/Allen_brain/test/')\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob #for selecting png files in training images folder\n",
    "from natsort import natsorted, ns #for sorting filenames in a directory\n",
    "import skimage\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[1536 1536]\n",
      " [ 768  768]\n",
      " [ 384  384]\n",
      " [ 192  192]\n",
      " [  96   96]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        61\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  6144\n",
      "IMAGE_MIN_DIM                  1536\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [6144 6144    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               61\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           brain\n",
      "NUM_CLASSES                    325\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                2000\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               100\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class BrainConfig(Config):\n",
    "    \"\"\"Configuration for training on the brain dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the brain dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"brain\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1 #8 ; reduced to avoid running out of memory when image size increased\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 324  # background + 8 regions\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 512*3 #128\n",
    "    IMAGE_MAX_DIM = 2048*3#128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 2000 #100 #steps_per_epoch: Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. \n",
    "                          #steps_per_epoch = TotalTrainingSamples / TrainingBatchSize (default to use entire training data per epoch; can modify if required)\n",
    "                          \n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 100 #5 #validation_steps = TotalvalidationSamples / ValidationBatchSize\n",
    "                         #Ideally, you use all your validation data at once. If you use only part of your validation data, you will get different metrics for each batch, \n",
    "                         #what may make you think that your model got worse or better when it actually didn't, you just measured different validation sets.\n",
    "                         #That's why they suggest validation_steps = uniqueValidationData / batchSize. \n",
    "                         #Theoretically, you test your entire data every epoch, as you theoretically should also train your entire data every epoch.\n",
    "                         #https://stackoverflow.com/questions/45943675/meaning-of-validation-steps-in-keras-sequential-fit-generator-parameter-list\n",
    "    \n",
    "\n",
    "    \n",
    "    ###### Further changes (experimentation):\n",
    "    \n",
    "     # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 61 #100 #decreased to avoid duplicate instances of each brain region\n",
    "    \n",
    "    # Max number of final detections\n",
    "    DETECTION_MAX_INSTANCES = 61 #100 # #decreased to avoid duplicate instances of each brain region\n",
    "\n",
    "    # Minimum probability value to accept a detected instance\n",
    "    # ROIs below this threshold are skipped\n",
    "    DETECTION_MIN_CONFIDENCE =  0.9 #0.7\n",
    "\n",
    "    # Non-maximum suppression threshold for detection\n",
    "    DETECTION_NMS_THRESHOLD = 0.3 # if overlap ratio is greater than the overlap threshold (0.3), suppress object (https://www.pyimagesearch.com/2014/11/17/non-maximum-suppression-object-detection-python)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "config = BrainConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load training dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the brain sections dataset, `load_brain()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference() # do not need to for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atlas_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68</td>\n",
       "      <td>Frontal_pole_layer_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>667</td>\n",
       "      <td>Frontal_pole_layer_23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>526157192</td>\n",
       "      <td>Frontal_pole_layer_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>526157196</td>\n",
       "      <td>Frontal_pole_layer_6a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>526322264</td>\n",
       "      <td>Frontal_pole_layer_6b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     atlas_id                   name\n",
       "7          68   Frontal_pole_layer_1\n",
       "8         667  Frontal_pole_layer_23\n",
       "9   526157192   Frontal_pole_layer_5\n",
       "10  526157196  Frontal_pole_layer_6a\n",
       "11  526322264  Frontal_pole_layer_6b"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_structues = (pd\n",
    "                 .read_csv(\"complet.csv\")\n",
    "                 .pipe(lambda x: x.loc[x.depth==7,['atlas_id','name']])\n",
    "                 .assign(name = lambda x: [ re.sub(r'\\W+','',re.sub(r'\\s+','_',str(i))) for i in x.name]))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "all_structues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Create training dataset:\n",
    "\n",
    "class BrainDataset_Train(utils.Dataset):\n",
    "    \"\"\"Generates the brain section dataset. The dataset consists of locally stored \n",
    "    brain section images, to which file access is required.\n",
    "    \"\"\"\n",
    "\n",
    "    #see utils.py for default def load_image() function; modify according to your dataset\n",
    "    \n",
    "    def load_brain(self): \n",
    "        \"\"\"\n",
    "        for naming image files follow this convention: '*_(image_id).jpg'\n",
    "        \"\"\"\n",
    "        \n",
    "        os.chdir('/home/hamza/Allen_brain/test/')\n",
    "        for idx, i in enumerate(all_structues.name.tolist()):\n",
    "            self.add_class('brain',str(idx),i)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        training_images_folder = 'Train/Training_JPG'\n",
    "        os.chdir(training_images_folder)\n",
    "        \n",
    "        cwd = os.getcwd()\n",
    "        img_list = glob.glob('*.jpg')\n",
    "        img_list = natsorted(img_list, key=lambda y: y.lower())\n",
    "        for i in img_list:  #image_ids start at 0 (to keep correspondence with load_mask which begins at image_id=0)!\n",
    "            img = skimage.io.imread(i) #grayscale = 0\n",
    "            im_dims = np.shape(img)\n",
    "            self.add_image(\"brain\", image_id=int(i[:-4]), path = cwd+'/'+i,height = im_dims[0], width = im_dims[1])#, depth = im_dims[2])\n",
    "            \n",
    "            #print(im_dims)\n",
    "      \n",
    "            \n",
    "    \n",
    "    def load_mask(self,image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\"\"\"\n",
    "        \n",
    "        os.chdir('/home/hamza/Allen_brain/test/')\n",
    "        print(image_id)\n",
    "        masks_folder = 'Train/Training_Mask'\n",
    "        os.chdir(masks_folder)\n",
    "        subfolder = str(self.image_info[image_id]['id'])#add 1 to image_id, to get to correct corresponding masks folder for a given image \n",
    "        print(subfolder)\n",
    "        os.chdir(subfolder) \n",
    "        \n",
    "        info = info = self.image_info[image_id]    \n",
    "        \n",
    "        \n",
    "        \n",
    "        mk_list = glob.glob('*.png')\n",
    "        \n",
    "        count = len(mk_list)\n",
    "        mk_id = 0\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)        \n",
    "        class_ids = np.zeros(count)\n",
    "        \n",
    "        for m in mk_list:\n",
    "            bin_mask = np.array(Image.open(m).convert('1')) # grayscale=0\n",
    "            mk_size = np.shape(bin_mask)\n",
    "            mask[:, :, mk_id]= bin_mask\n",
    "            \n",
    "            # Map class names to class IDs.\n",
    "            print(m)\n",
    "            class_ids[mk_id] = all_structues.loc[all_structues.name==m[:-9],'atlas_id'] #fifth last position from mask_image name = class_id #need to update(range) if class_ids become two/three-digit numbers \n",
    "            mk_id += 1\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "########### Create validation dataset:   \n",
    "\n",
    "class BrainDataset_Val(utils.Dataset):\n",
    "    \"\"\"Generates the brain section dataset. The dataset consists of locally stored \n",
    "    brain section images, to which file access is required.\n",
    "    \"\"\"\n",
    "\n",
    "    #see utils.py for default def load_image() function; modify according to your dataset\n",
    "    \n",
    "    def load_brain(self): \n",
    "        \"\"\"\n",
    "        for naming image files follow this convention: '*_(image_id+1).jpg'\n",
    "        \"\"\"\n",
    "        \n",
    "        os.chdir('/home/hamza/Allen_brain/test/')\n",
    "        for idx, i in enumerate(all_structues.name.tolist()):\n",
    "            self.add_class('brain',str(idx),i)\n",
    "        \n",
    "        \n",
    "        \n",
    "        training_images_folder = 'Val/Val_JPG'\n",
    "        os.chdir(training_images_folder)\n",
    "        \n",
    "        cwd = os.getcwd()\n",
    "        img_list = glob.glob('*.jpg')\n",
    "        img_list = natsorted(img_list, key=lambda y: y.lower())\n",
    "        for i in img_list:  #image_ids start at 0 (to keep correspondence with load_mask which begins at image_id=0)!\n",
    "            img = skimage.io.imread(i) #grayscale = 0\n",
    "            im_dims = np.shape(img)\n",
    "            self.add_image(\"brain\", image_id=int(i[:-4]), path = cwd+'/'+i,height = im_dims[0], width = im_dims[1])#, depth = im_dims[2])\n",
    "            \n",
    "            #print(im_dims)\n",
    "      \n",
    " \n",
    "    def load_mask(self,image_id):\n",
    "        \"\"\"Load instance masks for the given image.\n",
    "        Different datasets use different ways to store masks. This\n",
    "        function converts the different mask format to one format\n",
    "        in the form of a bitmap [height, width, instances].\n",
    "\n",
    "        Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\"\"\"\n",
    "        \n",
    "        os.chdir('/home/hamza/Allen_brain/test/')\n",
    "        print(image_id)\n",
    "        masks_folder = 'Val/Val_Mask'\n",
    "        os.chdir(masks_folder)\n",
    "        subfolder = str(self.image_info[image_id]['id'])#add 1 to image_id, to get to correct corresponding masks folder for a given image \n",
    "        print(subfolder)\n",
    "        os.chdir(subfolder) \n",
    "        \n",
    "        info = self.image_info[image_id]   \n",
    "        \n",
    "        \n",
    "        \n",
    "        mk_list = glob.glob('*.png')\n",
    "        \n",
    "        count = len(mk_list)\n",
    "        mk_id = 0\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)        \n",
    "        class_ids = np.zeros(count)\n",
    "        \n",
    "        for m in mk_list:\n",
    "            bin_mask = np.array(Image.open(m).convert('1')) # grayscale=0\n",
    "            mk_size = np.shape(bin_mask)\n",
    "            mask[:, :, mk_id]= bin_mask\n",
    "            \n",
    "            # Map class names to class IDs.\n",
    "            print(m)\n",
    "            class_ids[mk_id] = all_structues.loc[all_structues.name==m[:-9],'atlas_id'] #fifth last position from mask_image name = class_id #need to update(range) if class_ids become two/three-digit numbers \n",
    "            mk_id += 1\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = BrainDataset_Train()\n",
    "dataset_train.load_brain()\n",
    "dataset_train.prepare() #does nothing for now \n",
    "\n",
    "dataset_val = BrainDataset_Val()\n",
    "dataset_val.load_brain()\n",
    "dataset_val.prepare()#does nothing for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/hamza/virtualENV/python36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1208: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/hamza/virtualENV/python36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/hamza/Allen_brain/test/logs/brain20200120T1315/mask_rcnn_brain_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n",
      "0\n",
      "100960516\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4160, 4544, 3)\n",
      "WARNING:tensorflow:From /home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/hamza/virtualENV/python36/lib/python3.6/site-packages/keras/engine/training.py:2039: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "5\n",
      "100960496\n",
      "2\n",
      "100960484\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4416, 4608, 3)\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4352, 4672, 3)\n",
      "6\n",
      "100960500\n",
      "7\n",
      "100960504\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4032, 4288, 3)\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4816, 4992, 3)\n",
      "2\n",
      "100960484\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "6\n",
      "100960500\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_mitral_layer_mask.png\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4352, 4672, 3)\n",
      "Main_olfactory_bulb_outer_plexiform_layer_mask.png\n",
      "Main_olfactory_bulb_glomerular_layer_mask.png\n",
      "Main_olfactory_bulb_granule_layer_mask.png\n",
      "Main_olfactory_bulb_inner_plexiform_layer_mask.png\n",
      "original image shape:  (4816, 4992, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=2, \n",
    "            layers='heads') #epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=3, \n",
    "            layers=\"all\")#layers=\"heads\" ; epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
